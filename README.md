## Udacity - Data Engineering - 4
# Data Lake with AWS and Spark

## About / Synopsis

In this project, I apply what I've learned on Spark and data lakes to build an ETL pipeline for a data lake hosted on S3. To complete the project, I need to load data from S3, process the data into analytics tables using Spark, and load them back into S3.

## Table of Contents
* [Project Datasets](#project-datasets)
    - [Song Dataset](#song-dataset)
    - [Log Dataset](#log-dataset)
* [Schema for Song Play Analysis](#schema-for-song-play-analysis)
    - [Fact Table](#fact-table)
    - [Dimension Tables](#dimension-tables)
* [Files in the Project](#files-in-the-project)
* [Running the Project](#running-the-project)
* [What I Have Learned](#what-i-have-learned)
* [Author](#author)

## Project Datasets

I'll be working with two datasets that reside in S3. Here are the S3 links for each:
* Song data: `s3://udacity-dend/song_data`
* Log data: `s3://udacity-dend/log_data`

### Song Dataset

The first dataset is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, `song_data/A/B/C/TRABCEI128F424C983.json`

### Log Dataset

The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations. The log files in the dataset are partitioned by year and month. For example, `log_data/2018/11/2018-11-12-events.json`.

## Schema for Song Play Analysis

Using the song and log datasets, I will create a star schema optimized for queries on song play analysis. This includes the following tables. Each of the five tables are written to parquet files in a separate analytics directory on S3.

### Fact Table

* **songplays** - records in log data associated with song plays i.e. records with page `NextSong`
    - *songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent*
    - Song plays table files are partitioned by year and month.

### Dimension Tables

* **users** - users in the app
    - *user_id, first_name, last_name, gender, level*
* **songs** - songs in music database
    - *song_id, title, artist_id, year, duration*
    - Songs table files are partitioned by year and then artist.
* **artists** - artists in music database
    - *artist_id, name, location, latitude, longitude*
* **time** - timestamps of records in songplays broken down into specific units
    - *start_time, hour, day, week, month, year, weekday*
    - Time table files are partitioned by year and month.

## Files in the Project

- `dl.cfg` is a configuration file, which you need to complete with your AWS keys and data paths.
- `etl_test.ipynb` tests the ETL following the procedure step by step.
- `sparkify.ipynb` investigates data of the full `s3://udacity-dend/` dataset on the standalone Spark cluster.
- `etl.py` loads data from S3, process the data into analytics tables using Spark, and load them back into S3.
- `README.md` provides the project description you are now reading.

## Running the Project

1) Open `dl.cfg` and fill in your AWS keys and your data input and output paths.
1) You can either run `etl.py` to create and populate the data lake, or
1) You can run `etl_test.ipynb` to test the ETL procedure step by step.

## What I Have Learned

Through the implementation of this project, while solving the project's core tasks, I've learned:

1) Setting up and accessing the Spark cluster on AWS EMR.
1) Data modeling for Spark.
1) Reading from S3 into Spark and writing back to S3. The writing part was feeling like the bottleneck of my ETL process so I used `.limit(5)` on my data frames when testing.
1) ETL on Spark with data wrangling, extraction and type conversions.
1) Data investigation with Spark queries.

## Author

Andrii Dzugaiev, [in:dzugaev](https://www.linkedin.com/in/dzugaev/)